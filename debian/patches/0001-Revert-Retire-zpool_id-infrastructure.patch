From 786ba6f06aec144dd126318e6a0f751c6b307af5 Mon Sep 17 00:00:00 2001
From: Darik Horn <dajhorn@vanadac.com>
Date: Sat, 2 Feb 2013 16:31:47 -0600
Subject: [PATCH] Revert "Retire zpool_id infrastructure"

On managed Debian installations, `/etc/zfs/zdev.conf` is a registered
configuration file, so we must carry this patch until a debconf for converting
it to the new `/etc/zfs/vdev.conf` syntax is implemented.

This reverts commit dbf763b39b232996f0d6bb0022e4446643c18e05.
---
 PKGBUILD-zfs.in                       |    1 +
 cmd/Makefile.am                       |    2 +-
 cmd/zpool_id/Makefile.am              |    1 +
 cmd/zpool_id/zpool_id                 |   86 ++++++++++
 cmd/zpool_layout/Makefile.am          |    1 +
 cmd/zpool_layout/zpool_layout         |  281 +++++++++++++++++++++++++++++++++
 configure.ac                          |    3 +
 dracut/90zfs/module-setup.sh.in       |    4 +-
 etc/zfs/Makefile.am                   |    6 +-
 etc/zfs/zdev.conf                     |    9 ++
 etc/zfs/zdev.conf.dragon.example      |  176 +++++++++++++++++++++
 etc/zfs/zdev.conf.supermicro.example  |   30 ++++
 etc/zfs/zdev.conf.x4550.example       |   66 ++++++++
 include/libzfs.h                      |    2 +-
 lib/libzfs/libzfs_import.c            |    1 +
 scripts/Makefile.am                   |    2 +-
 scripts/common.sh.in                  |    5 +-
 scripts/zpool-config/zpool-raid0.sh   |    7 +-
 scripts/zpool-config/zpool-raid10.sh  |    7 +-
 scripts/zpool-config/zpool-raidz.sh   |    7 +-
 scripts/zpool-layout/Makefile.am      |   22 +++
 scripts/zpool-layout/dragon.ddn.conf  |   67 ++++++++
 scripts/zpool-layout/dragon.llnl.conf |   67 ++++++++
 udev/rules.d/60-zpool.rules.in        |    9 ++
 udev/rules.d/Makefile.am              |    2 +
 zfs-script-config.sh.in               |    1 +
 zfs.spec.in                           |    2 +
 27 files changed, 851 insertions(+), 16 deletions(-)
 create mode 100644 cmd/zpool_id/Makefile.am
 create mode 100755 cmd/zpool_id/zpool_id
 create mode 100644 cmd/zpool_layout/Makefile.am
 create mode 100755 cmd/zpool_layout/zpool_layout
 create mode 100644 etc/zfs/zdev.conf
 create mode 100644 etc/zfs/zdev.conf.dragon.example
 create mode 100644 etc/zfs/zdev.conf.supermicro.example
 create mode 100644 etc/zfs/zdev.conf.x4550.example
 create mode 100644 scripts/zpool-layout/Makefile.am
 create mode 100644 scripts/zpool-layout/dragon.ddn.conf
 create mode 100644 scripts/zpool-layout/dragon.llnl.conf
 create mode 100644 udev/rules.d/60-zpool.rules.in

--- /dev/null
+++ b/cmd/zpool_id/Makefile.am
@@ -0,0 +1 @@
+dist_udev_SCRIPTS = zpool_id
--- /dev/null
+++ b/cmd/zpool_id/zpool_id
@@ -0,0 +1,86 @@
+#!/bin/sh
+
+CONFIG="${CONFIG:-/etc/zfs/zdev.conf}"
+
+if [ -z "${PATH_ID}" ]; then
+	# The path_id helper became a builtin command in udev 174.
+	if [ -x '/lib/udev/path_id' ]; then
+		PATH_ID='/lib/udev/path_id'
+	else
+		PATH_ID='udevadm test-builtin path_id'
+	fi
+fi
+
+die() {
+	echo "Error: $*"
+	exit 1
+}
+
+usage() {
+	cat << EOF
+Usage: zpool_id [-h] [-c configfile] <devpath>
+  -c    Alternate config file [default /etc/zfs/zdev.conf]
+  -d    Use path_id from device as the mapping key
+  -h    Show this message
+EOF
+	exit 1
+}
+
+while getopts 'c:d:h' OPTION; do
+	case ${OPTION} in
+	c)
+		CONFIG="${OPTARG}"
+		;;
+	d)
+		DEVICE="${OPTARG}"
+		;;
+	h)
+		usage
+		;;
+	esac
+done
+
+# Check that a device was requested
+[ -z "${DEVICE}" ] && usage
+
+# Check for the existence of a configuration file
+[ ! -f "${CONFIG}" ] && die "Missing config file: ${CONFIG}"
+
+# If we are handling a multipath device then $DM_UUID will be
+# exported and we'll use its value (prefixed with dm-uuid per
+# multipathd's naming convention) as our unique persistent key.
+# For traditional devices we'll obtain the key from udev's
+# path_id.
+if [ -n "${DM_UUID}" ] && echo "${DM_UUID}" | grep -q -e '^mpath' ; then
+	ID_PATH="dm-uuid-${DM_UUID}"
+else
+	eval `${PATH_ID} ${DEVICE}`
+	[ -z "${ID_PATH}" ] && die "Missing ID_PATH for ${DEVICE}"
+fi
+
+# Use the persistent key to lookup the zpool device id in the
+# configuration file which is of the format <device id> <key>.
+# Lines starting with #'s are treated as comments and ignored.
+# Exact matches are required, wild cards are not supported,
+# and only the first match is returned.
+ID_ZPOOL=''
+while read CONFIG_ZPOOL CONFIG_PATH REPLY; do
+	if [ "${CONFIG_ZPOOL}" != "${CONFIG_ZPOOL#\#}" ]; then
+		# Skip comment lines.
+		continue
+	fi
+	if [ "${CONFIG_PATH}" = "${ID_PATH}" ]; then
+		ID_ZPOOL="${CONFIG_ZPOOL}"
+		break
+	fi
+done <"${CONFIG}"
+
+[ -z "${ID_ZPOOL}" ] && die "Missing ID_ZPOOL for ID_PATH: ${ID_PATH}"
+
+if [ -n "${ID_ZPOOL}" ]; then
+	echo "ID_PATH=${ID_PATH}"
+	echo "ID_ZPOOL=${ID_ZPOOL}"
+	echo "ID_ZPOOL_PATH=disk/zpool/${ID_ZPOOL}"
+fi
+
+exit 0
--- /dev/null
+++ b/cmd/zpool_layout/Makefile.am
@@ -0,0 +1 @@
+dist_bin_SCRIPTS = zpool_layout
--- /dev/null
+++ b/cmd/zpool_layout/zpool_layout
@@ -0,0 +1,281 @@
+#!/bin/bash
+#
+# Direct-Attached Mode
+# --------------------
+# Set BUSES and HOST_PORTS to match the topology of your system.  As
+# each port is enumerated it will be assigned the next channel name.
+# The current script enumerates each port on a bus before moving on
+# to enumerate the next bus.
+#
+# Every distribution, version of udev, and type of attached storage
+# seems to result in slightly different formatting of the by-path
+# name.   For this reason you may need to adjust the parsing below
+# to suit your needs.  This is one of the reasons to use a custom
+# /etc/zfs/zdev.conf file, it allows the by-path naming convention
+# to change and still keep the simple <channel><rank> naming.
+#
+# SAS-Switch Mode
+# -------------------------
+# When the host accesses disk via SAS switches the combination of
+# bus and port number does not necessarily uniquely identify a
+# channel or disk drawer.  In this case we must resort to other
+# means to infer the physical topology.  For a single-level network
+# (i.e. no switch cascading) we can assign alphabetic channel labels
+# based on the switch port number that the drawer is connected to.
+# If support for more complex topologies is needed this script will
+# need to be customized or replaced.
+#
+# In SAS-Switch mode (enabled with "-g switch" ) we require that
+# udev has been configured to create per-disk symbolic links in
+# /dev/disk/by-id of the form
+# <label>-<UUID>-switch-port:<X>-slot:<Y>.  <label> is a string that
+# depends on the subsystem that created the link and defaults to
+# "dm-uuid-mpath" (this prefix is used by multipathd).  <UUID> is a
+# unique identifier for the disk typically obtained from the scsi_id
+# program.  <X> and <Y> denote the switch port and disk slot
+# numbers, respectively, and are typically obtained from sysfs.
+
+AWK=${AWK:-/usr/bin/awk}
+CONFIG=${CONFIG:-/etc/zfs/zdev.conf}
+BUSES=( 01 02 03 )
+HOST_PORTS=( 4 0 )
+SWITCH_PORTS=( 0 1 2 3 4 5 6 7 8 9 )
+CHANNELS=( A B C D E F G H I J K L M N O P Q R S T U V W X Y Z )
+TOPOLOGY="direct"
+TRIGGER="no"
+MAPPING=linux
+LABEL=${LABEL:-"dm-uuid-mpath"}
+DEV_DISK_DIR="/dev/disk/by-path"
+
+shopt -s extglob
+
+usage() {
+	cat << EOF
+Usage: zpool_layout [-th] [-c file] [-b buses] [-o switch_ports]
+           [-p host_ports] [-n channels] [-m map] [-l label]
+           [-g direct|switch]
+  -c    Alternate config file [default=${CONFIG}]
+  -b    Enumerate buses [default="${BUSES[*]}"]
+  -o    Enumerate switch ports [default="${SWITCH_PORTS[*]}"]
+  -p    Enumerate host ports [default="${HOST_PORTS[*]}"]
+  -n    Channel names [default="A..Z"]
+  -g    Storage network topology [default="${TOPOLOGY}"]
+  -t    Trigger and wait for udev to settle [default=${TRIGGER}]
+  -l    Prefix of SAS-switch-mode device links [default=${LABEL}]
+  -m    Slot mapping [default=${MAPPING}]
+  -h    Show this message
+EOF
+	exit 0
+}
+
+while getopts 'c:b:o:p:n:l:m:g:th' OPTION; do
+	case ${OPTION} in
+	c)
+		CONFIG=${OPTARG}
+		;;
+	b)
+		BUSES=(${OPTARG})
+		;;
+	o)
+		SWITCH_PORTS=(${OPTARG})
+		;;
+	p)
+		HOST_PORTS=(${OPTARG})
+		;;
+	n)
+		CHANNELS=(${OPTARG})
+		;;
+	l)
+		LABEL=${OPTARG}
+		;;
+	m)
+		MAPPING=`readlink -e ${OPTARG}`
+		;;
+	g)
+		TOPOLOGY=${OPTARG}
+		;;
+	t)
+		TRIGGER=yes
+		;;
+	h)
+		usage
+		;;
+	esac
+done
+
+# Verify mapping file exists if specified.
+# Linux-Slot	Custom-Slot
+if [ ${MAPPING} != "linux" ] && [ ! -e ${MAPPING} ]; then
+	echo "Error: Mapping file '${MAPPING}' does not exist"
+	exit 1
+fi
+
+# Save stdout as fd #8, then redirect stdout to the config file.
+exec 8>&1
+exec >${CONFIG}
+
+map_slot() {
+	local LINUX_SLOT=$1
+	local MAPPED_SLOT=
+
+	if [ ${MAPPING} = "linux" ]; then
+		MAPPED_SLOT=${LINUX_SLOT}
+	else
+		MAPPED_SLOT=`${AWK} "\\$1 == ${LINUX_SLOT} && !/^#/ \
+			{ print \\$2; exit }" $MAPPING`
+	fi
+	printf "%d" ${MAPPED_SLOT}
+}
+
+# Generate host port layout table for comment header.
+print_host_port_layout() {
+	echo "# ------------------ Host Port Layout ---------------------"
+	echo -n "#          "
+	for (( i=0; i<${#BUSES[*]}; i++ )); do
+		printf "%-8d" ${BUSES[$i]}
+	done
+	echo
+
+	for (( i=0, k=0; i<${#HOST_PORTS[*]}; i++ )); do
+		printf "# Port %-2d  " ${HOST_PORTS[$i]}
+
+		for (( j=0; j<${#BUSES[*]}; j++, k++ )); do
+			let k=$j*${#HOST_PORTS[*]}+$i
+			printf "%-8s" ${CHANNELS[$k]}
+		done
+		echo
+	done
+	echo "#"
+}
+
+# Generate SAS switch port layout table for comment header.
+print_switch_port_layout() {
+	echo "# --------------- SAS Switch Port Layout ------------------"
+	echo -n "# Switch Port   "
+	for (( i=0; i<${#SWITCH_PORTS[*]}; i++ )); do
+		printf "%3d" ${SWITCH_PORTS[$i]}
+	done
+	echo
+	echo -n "# Channel       "
+	for (( i=0; i<${#SWITCH_PORTS[*]}; i++ )); do
+		printf "%3s" ${CHANNELS[$i]}
+	done
+	echo
+	echo "#"
+}
+
+# Generate channel/disk layout table for comment header.
+print_channel_layout() {
+	pushd ${DEV_DISK_DIR} >/dev/null
+	echo "# ----------------- Channel/Disk Layout -------------------"
+	echo "# Channel  Disks"
+	if [ ${TOPOLOGY} = "switch" ] ; then
+		for (( i=0; i<${#SWITCH_PORTS[*]}; i++ )); do
+			printf "# %-9s" ${CHANNELS[$i]}
+			p=${SWITCH_PORTS[$i]}
+			ls ${LABEL}-+([0-9a-f])-switch-port:${p}-slot:+([0-9]) \
+				2>/dev/null | cut -f3 -d':' | sort -u -n | \
+				xargs | tr ' ' ','
+		done
+	else
+		for (( i=0, k=0; i<${#BUSES[*]}; i++ )); do
+			for (( j=0; j<${#HOST_PORTS[*]}; j++, k++ )); do
+				printf "# %-9s" ${CHANNELS[$k]}
+				ls *:${BUSES[$i]}:*:${HOST_PORTS[$j]}* \
+					2>/dev/null | cut -f7 -d'-' | \
+					sort -u -n | xargs | tr ' ' ','
+			done
+		done
+	fi
+	echo "#"
+	popd > /dev/null
+}
+
+# Generate mapping from <channel><rank> to by-path name.
+map_shortname_to_by_path() {
+	pushd ${DEV_DISK_DIR} >/dev/null
+	for (( i=0, k=0; i<${#BUSES[*]}; i++ )); do
+		for (( j=0; j<${#HOST_PORTS[*]}; j++, k++ )); do
+			BYPATH=(`ls *:${BUSES[$i]}:*:${HOST_PORTS[$j]}* \
+				2>/dev/null | grep -v part | \
+				sort -n -k7 -t'-' | cut -f1-6 -d'-'`)
+			SLOTS=(`ls *:${BUSES[$i]}:*:${HOST_PORTS[$j]}* \
+				2>/dev/null | grep -v part | \
+				sort -n -k7 -t'-' | cut -f7 -d'-'`)
+			TMP_FILE=`mktemp`
+
+			for (( l=0; l<${#SLOTS[*]}; l++ )); do
+				MAPPED_SLOT=`map_slot ${SLOTS[$l]}`
+				printf "%s%d\t%s-%d\n" \
+					${CHANNELS[$k]} ${MAPPED_SLOT} \
+					${BYPATH[$l]} ${SLOTS[$l]} >>${TMP_FILE}
+			done
+
+			echo
+			echo -n "# Channel ${CHANNELS[$k]}, "
+			echo "Bus ${BUSES[$i]}, Port ${HOST_PORTS[$j]}"
+			cat ${TMP_FILE} | sort -n -k2 -t${CHANNELS[$k]}
+			rm -f ${TMP_FILE}
+		done
+	done
+	popd >/dev/null
+}
+
+# Generate mapping from <channel><rank> to by-id name.
+map_shortname_to_by_id() {
+	pushd ${DEV_DISK_DIR} >/dev/null
+	for (( i=0; i<${#SWITCH_PORTS[*]}; i++ )); do
+		p=${SWITCH_PORTS[$i]}
+		BYID=(`ls ${LABEL}-+([0-9a-f])-switch-port:${p}-slot:+([0-9]) \
+			2>/dev/null | grep -v part | sort -k3n -t':' | \
+			cut -f1-2 -d':'`)
+		SLOTS=(`ls ${LABEL}-+([0-9a-f])-switch-port:${p}-slot:+([0-9]) \
+			2>/dev/null | grep -v part | sort -k3n -t':' | \
+			cut -f3 -d':'`)
+		TMP_FILE=`mktemp`
+
+		for (( l=0; l<${#SLOTS[*]}; l++ )); do
+			MAPPED_SLOT=`map_slot ${SLOTS[$l]}`
+			printf "%s%d\t%s:%d\n" \
+				${CHANNELS[$i]} ${MAPPED_SLOT} ${BYID[$l]} \
+				${SLOTS[$l]} >>${TMP_FILE}
+		done
+
+		echo
+		echo -n "# Channel ${CHANNELS[$i]}, "
+		echo "SAS Switch Port ${SWITCH_PORTS[$i]}"
+		cat ${TMP_FILE} | sort -n -k2 -t${CHANNELS[$i]}
+		rm -f ${TMP_FILE}
+	done
+	popd > /dev/null
+}
+
+# Generate comment header.
+echo "#"
+echo "# Custom ${DEV_DISK_DIR}  to /dev/disk/zpool mapping, "
+echo "# based of the following physical cable layout."
+echo "#"
+
+case ${TOPOLOGY} in
+	direct)
+		print_host_port_layout
+		print_channel_layout
+		map_shortname_to_by_path
+		;;
+	switch)
+		DEV_DISK_DIR="/dev/disk/by-id"
+		print_switch_port_layout
+		print_channel_layout
+		map_shortname_to_by_id
+		;;
+esac
+
+# Restore stdout from fd #8 and close fd #8.
+exec 1>&8 8>&-
+
+if [ ${TRIGGER} = "yes" ]; then
+	udevadm trigger --action=change --subsystem-match=block
+	udevadm settle
+fi
+
+exit 0
--- a/configure.ac
+++ b/configure.ac
@@ -106,7 +106,9 @@
 	cmd/zpios/Makefile
 	cmd/mount_zfs/Makefile
 	cmd/fsck_zfs/Makefile
+	cmd/zpool_layout/Makefile
 	cmd/zvol_id/Makefile
+	cmd/zpool_id/Makefile
 	cmd/vdev_id/Makefile
 	cmd/arcstat/Makefile
 	cmd/dbufstat/Makefile
@@ -130,6 +132,7 @@
 	scripts/zpios-profile/Makefile
 	scripts/zpios-test/Makefile
 	scripts/zpool-config/Makefile
+	scripts/zpool-layout/Makefile
 	scripts/common.sh
 	rpm/Makefile
 	rpm/fedora/Makefile
--- a/etc/zfs/Makefile.am
+++ b/etc/zfs/Makefile.am
@@ -4,6 +4,10 @@
 	vdev_id.conf.alias.example \
 	vdev_id.conf.sas_direct.example \
 	vdev_id.conf.sas_switch.example \
-	vdev_id.conf.multipath.example
+	vdev_id.conf.multipath.example \
+	zdev.conf \
+	zdev.conf.supermicro.example \
+	zdev.conf.dragon.example \
+	zdev.conf.x4550.example
 
 EXTRA_DIST = $(pkgsysconf_DATA)
--- /dev/null
+++ b/etc/zfs/zdev.conf
@@ -0,0 +1,9 @@
+#
+# Custom by-path mapping for large JBOD configurations
+#
+# Example Config: Desktop
+#
+
+#<ID>	<by-path name>
+#disk1	pci-0000:61:06.1-scsi-0:0:0:0
+#disk2	pci-0000:61:06.1-scsi-0:0:0:1
--- /dev/null
+++ b/etc/zfs/zdev.conf.dragon.example
@@ -0,0 +1,176 @@
+#
+# Custom by-path mapping for large JBOD configurations
+#
+# Example Config:
+# Quarter scale dragon drawer based SSU for RHEL6
+#
+
+# Channel A: PCI Bus 3, Enclosure 0x50001ff10e991000
+a1	pci-0000:03:00.0-sas-0x50001ff10e991000-34
+a2	pci-0000:03:00.0-sas-0x50001ff10e991000-35
+a3	pci-0000:03:00.0-sas-0x50001ff10e991000-39
+a4	pci-0000:03:00.0-sas-0x50001ff10e991000-40
+a5	pci-0000:03:00.0-sas-0x50001ff10e991000-44
+a6	pci-0000:03:00.0-sas-0x50001ff10e991000-45
+a7	pci-0000:03:00.0-sas-0x50001ff10e991000-48
+a8	pci-0000:03:00.0-sas-0x50001ff10e991000-49
+a9	pci-0000:03:00.0-sas-0x50001ff10e991000-50
+a10	pci-0000:03:00.0-sas-0x50001ff10e991000-53
+a11	pci-0000:03:00.0-sas-0x50001ff10e991000-54
+a12	pci-0000:03:00.0-sas-0x50001ff10e991000-55
+a13	pci-0000:03:00.0-sas-0x50001ff10e991000-58
+a14	pci-0000:03:00.0-sas-0x50001ff10e991000-59
+a15	pci-0000:03:00.0-sas-0x50001ff10e991000-60
+
+# Channel B: PCI Bus 3, Enclosure 0x50001ff10e9d1000
+b1	pci-0000:03:00.0-sas-0x50001ff10e9d1000-34
+b2	pci-0000:03:00.0-sas-0x50001ff10e9d1000-35
+b3	pci-0000:03:00.0-sas-0x50001ff10e9d1000-39
+b4	pci-0000:03:00.0-sas-0x50001ff10e9d1000-40
+b5	pci-0000:03:00.0-sas-0x50001ff10e9d1000-44
+b6	pci-0000:03:00.0-sas-0x50001ff10e9d1000-45
+b7	pci-0000:03:00.0-sas-0x50001ff10e9d1000-48
+b8	pci-0000:03:00.0-sas-0x50001ff10e9d1000-49
+b9	pci-0000:03:00.0-sas-0x50001ff10e9d1000-50
+b10	pci-0000:03:00.0-sas-0x50001ff10e9d1000-53
+b11	pci-0000:03:00.0-sas-0x50001ff10e9d1000-54
+b12	pci-0000:03:00.0-sas-0x50001ff10e9d1000-55
+b13	pci-0000:03:00.0-sas-0x50001ff10e9d1000-58
+b14	pci-0000:03:00.0-sas-0x50001ff10e9d1000-59
+b15	pci-0000:03:00.0-sas-0x50001ff10e9d1000-60
+
+# Channel C: PCI Bus 4, Enclosure 0x50001ff10e991000
+c1	pci-0000:04:00.0-sas-0x50001ff10e991000-31
+c2	pci-0000:04:00.0-sas-0x50001ff10e991000-32
+c3	pci-0000:04:00.0-sas-0x50001ff10e991000-33
+c4	pci-0000:04:00.0-sas-0x50001ff10e991000-36
+c5	pci-0000:04:00.0-sas-0x50001ff10e991000-37
+c6	pci-0000:04:00.0-sas-0x50001ff10e991000-38
+c7	pci-0000:04:00.0-sas-0x50001ff10e991000-41
+c8	pci-0000:04:00.0-sas-0x50001ff10e991000-42
+c9	pci-0000:04:00.0-sas-0x50001ff10e991000-43
+c10	pci-0000:04:00.0-sas-0x50001ff10e991000-46
+c11	pci-0000:04:00.0-sas-0x50001ff10e991000-47
+c12	pci-0000:04:00.0-sas-0x50001ff10e991000-51
+c13	pci-0000:04:00.0-sas-0x50001ff10e991000-52
+c14	pci-0000:04:00.0-sas-0x50001ff10e991000-56
+c15	pci-0000:04:00.0-sas-0x50001ff10e991000-57
+
+# Channel D: PCI Bus 4, Enclosure 0x50001ff10e9d1000
+d1	pci-0000:04:00.0-sas-0x50001ff10e9d1000-31
+d2	pci-0000:04:00.0-sas-0x50001ff10e9d1000-32
+d3	pci-0000:04:00.0-sas-0x50001ff10e9d1000-33
+d4	pci-0000:04:00.0-sas-0x50001ff10e9d1000-36
+d5	pci-0000:04:00.0-sas-0x50001ff10e9d1000-37
+d6	pci-0000:04:00.0-sas-0x50001ff10e9d1000-38
+d7	pci-0000:04:00.0-sas-0x50001ff10e9d1000-41
+d8	pci-0000:04:00.0-sas-0x50001ff10e9d1000-42
+d9	pci-0000:04:00.0-sas-0x50001ff10e9d1000-43
+d10	pci-0000:04:00.0-sas-0x50001ff10e9d1000-46
+d11	pci-0000:04:00.0-sas-0x50001ff10e9d1000-47
+d12	pci-0000:04:00.0-sas-0x50001ff10e9d1000-51
+d13	pci-0000:04:00.0-sas-0x50001ff10e9d1000-52
+d14	pci-0000:04:00.0-sas-0x50001ff10e9d1000-56
+d15	pci-0000:04:00.0-sas-0x50001ff10e9d1000-57
+
+# Channel E: PCI Bus 84, Enclosure 0x50001ff10e991000
+e1	pci-0000:84:00.0-sas-0x50001ff10e991000-3
+e2	pci-0000:84:00.0-sas-0x50001ff10e991000-4
+e3	pci-0000:84:00.0-sas-0x50001ff10e991000-5
+e4	pci-0000:84:00.0-sas-0x50001ff10e991000-8
+e5	pci-0000:84:00.0-sas-0x50001ff10e991000-9
+e6	pci-0000:84:00.0-sas-0x50001ff10e991000-10
+e7	pci-0000:84:00.0-sas-0x50001ff10e991000-13
+e8	pci-0000:84:00.0-sas-0x50001ff10e991000-14
+e9	pci-0000:84:00.0-sas-0x50001ff10e991000-15
+e10	pci-0000:84:00.0-sas-0x50001ff10e991000-19
+e11	pci-0000:84:00.0-sas-0x50001ff10e991000-20
+e12	pci-0000:84:00.0-sas-0x50001ff10e991000-24
+e13	pci-0000:84:00.0-sas-0x50001ff10e991000-25
+e14	pci-0000:84:00.0-sas-0x50001ff10e991000-29
+e15	pci-0000:84:00.0-sas-0x50001ff10e991000-30
+
+# Channel F: PCI Bus 84, Enclosure 0x50001ff10e9d1000
+f1	pci-0000:84:00.0-sas-0x50001ff10e9d1000-3
+f2	pci-0000:84:00.0-sas-0x50001ff10e9d1000-4
+f3	pci-0000:84:00.0-sas-0x50001ff10e9d1000-5
+f4	pci-0000:84:00.0-sas-0x50001ff10e9d1000-8
+f5	pci-0000:84:00.0-sas-0x50001ff10e9d1000-9
+f6	pci-0000:84:00.0-sas-0x50001ff10e9d1000-10
+f7	pci-0000:84:00.0-sas-0x50001ff10e9d1000-13
+f8	pci-0000:84:00.0-sas-0x50001ff10e9d1000-14
+f9	pci-0000:84:00.0-sas-0x50001ff10e9d1000-15
+f10	pci-0000:84:00.0-sas-0x50001ff10e9d1000-19
+f11	pci-0000:84:00.0-sas-0x50001ff10e9d1000-20
+f12	pci-0000:84:00.0-sas-0x50001ff10e9d1000-24
+f13	pci-0000:84:00.0-sas-0x50001ff10e9d1000-25
+f14	pci-0000:84:00.0-sas-0x50001ff10e9d1000-29
+f15	pci-0000:84:00.0-sas-0x50001ff10e9d1000-30
+
+# Channel G: PCI Bus 85, Enclosure 0x50001ff10e991000
+g1	pci-0000:85:00.0-sas-0x50001ff10e991000-1
+g2	pci-0000:85:00.0-sas-0x50001ff10e991000-2
+g3	pci-0000:85:00.0-sas-0x50001ff10e991000-6
+g4	pci-0000:85:00.0-sas-0x50001ff10e991000-7
+g5	pci-0000:85:00.0-sas-0x50001ff10e991000-11
+g6	pci-0000:85:00.0-sas-0x50001ff10e991000-12
+g7	pci-0000:85:00.0-sas-0x50001ff10e991000-16
+g8	pci-0000:85:00.0-sas-0x50001ff10e991000-17
+g9	pci-0000:85:00.0-sas-0x50001ff10e991000-18
+g10	pci-0000:85:00.0-sas-0x50001ff10e991000-21
+g11	pci-0000:85:00.0-sas-0x50001ff10e991000-22
+g12	pci-0000:85:00.0-sas-0x50001ff10e991000-23
+g13	pci-0000:85:00.0-sas-0x50001ff10e991000-26
+g14	pci-0000:85:00.0-sas-0x50001ff10e991000-27
+g15	pci-0000:85:00.0-sas-0x50001ff10e991000-28
+
+# Channel H: PCI Bus 85, Enclosure 0x50001ff10e9d1000
+h1	pci-0000:85:00.0-sas-0x50001ff10e9d1000-1
+h2	pci-0000:85:00.0-sas-0x50001ff10e9d1000-2
+h3	pci-0000:85:00.0-sas-0x50001ff10e9d1000-6
+h4	pci-0000:85:00.0-sas-0x50001ff10e9d1000-7
+h5	pci-0000:85:00.0-sas-0x50001ff10e9d1000-11
+h6	pci-0000:85:00.0-sas-0x50001ff10e9d1000-12
+h7	pci-0000:85:00.0-sas-0x50001ff10e9d1000-16
+h8	pci-0000:85:00.0-sas-0x50001ff10e9d1000-17
+h9	pci-0000:85:00.0-sas-0x50001ff10e9d1000-18
+h10	pci-0000:85:00.0-sas-0x50001ff10e9d1000-21
+h11	pci-0000:85:00.0-sas-0x50001ff10e9d1000-22
+h12	pci-0000:85:00.0-sas-0x50001ff10e9d1000-23
+h13	pci-0000:85:00.0-sas-0x50001ff10e9d1000-26
+h14	pci-0000:85:00.0-sas-0x50001ff10e9d1000-27
+h15	pci-0000:85:00.0-sas-0x50001ff10e9d1000-28
+
+# Channel I: PCI Bus 83, Enclosure 0x5000155359b8e33f
+i1	pci-0000:83:00.0-sas-0x5000155359b8e33f-0
+i2	pci-0000:83:00.0-sas-0x5000155359b8e33f-1
+i3	pci-0000:83:00.0-sas-0x5000155359b8e33f-2
+i4	pci-0000:83:00.0-sas-0x5000155359b8e33f-3
+i5	pci-0000:83:00.0-sas-0x5000155359b8e33f-4
+i6	pci-0000:83:00.0-sas-0x5000155359b8e33f-5
+i7	pci-0000:83:00.0-sas-0x5000155359b8e33f-6
+i8	pci-0000:83:00.0-sas-0x5000155359b8e33f-7
+i9	pci-0000:83:00.0-sas-0x5000155359b8e33f-8
+i10	pci-0000:83:00.0-sas-0x5000155359b8e33f-9
+i11	pci-0000:83:00.0-sas-0x5000155359b8e33f-10
+i12	pci-0000:83:00.0-sas-0x5000155359b8e33f-11
+i13	pci-0000:83:00.0-sas-0x5000155359b8e33f-12
+i14	pci-0000:83:00.0-sas-0x5000155359b8e33f-13
+i15	pci-0000:83:00.0-sas-0x5000155359b8e33f-14
+
+# Channel J: PCI Bus 83, Enclosure 0x5000155359dba33f
+j1	pci-0000:83:00.0-sas-0x5000155359dba33f-0
+j2	pci-0000:83:00.0-sas-0x5000155359dba33f-1
+j3	pci-0000:83:00.0-sas-0x5000155359dba33f-2
+j4	pci-0000:83:00.0-sas-0x5000155359dba33f-3
+j5	pci-0000:83:00.0-sas-0x5000155359dba33f-4
+j6	pci-0000:83:00.0-sas-0x5000155359dba33f-5
+j7	pci-0000:83:00.0-sas-0x5000155359dba33f-6
+j8	pci-0000:83:00.0-sas-0x5000155359dba33f-7
+j9	pci-0000:83:00.0-sas-0x5000155359dba33f-8
+j10	pci-0000:83:00.0-sas-0x5000155359dba33f-9
+j11	pci-0000:83:00.0-sas-0x5000155359dba33f-10
+j12	pci-0000:83:00.0-sas-0x5000155359dba33f-11
+j13	pci-0000:83:00.0-sas-0x5000155359dba33f-12
+j14	pci-0000:83:00.0-sas-0x5000155359dba33f-13
+j15	pci-0000:83:00.0-sas-0x5000155359dba33f-14
--- /dev/null
+++ b/etc/zfs/zdev.conf.supermicro.example
@@ -0,0 +1,30 @@
+#
+# Custom by-path mapping for large JBOD configurations
+#
+# Example Config:
+# Single Supermicro JBOD for RHEL6
+#
+
+# Channel A: PCI Bus 7, Enclosure 0x500304800027367f
+a1	pci-0000:07:00.0-sas-0x500304800027367f-0
+a2	pci-0000:07:00.0-sas-0x500304800027367f-1
+a3	pci-0000:07:00.0-sas-0x500304800027367f-2
+a4	pci-0000:07:00.0-sas-0x500304800027367f-3
+
+# Channel B: PCI Bus 7, Enclosure 0x500304800027367f
+b1	pci-0000:07:00.0-sas-0x500304800027367f-4
+b2	pci-0000:07:00.0-sas-0x500304800027367f-5
+b3	pci-0000:07:00.0-sas-0x500304800027367f-6
+b4	pci-0000:07:00.0-sas-0x500304800027367f-7
+
+# Channel C: PCI Bus 7, Enclosure 0x500304800027367f
+c1	pci-0000:07:00.0-sas-0x500304800027367f-8
+c2	pci-0000:07:00.0-sas-0x500304800027367f-9
+c3	pci-0000:07:00.0-sas-0x500304800027367f-10
+c4	pci-0000:07:00.0-sas-0x500304800027367f-11
+
+# Channel D: PCI Bus 7, Enclosure 0x500304800027367f
+d1	pci-0000:07:00.0-sas-0x500304800027367f-12
+d2	pci-0000:07:00.0-sas-0x500304800027367f-13
+d3	pci-0000:07:00.0-sas-0x500304800027367f-14
+d4	pci-0000:07:00.0-sas-0x500304800027367f-15
--- /dev/null
+++ b/etc/zfs/zdev.conf.x4550.example
@@ -0,0 +1,66 @@
+#
+# Custom by-path mapping for large JBOD configurations
+#
+# Example Config:
+# Sun x4550 for RHEL5
+#
+
+# Channel A: PCI Bus 2
+a1	pci-0000:02:00.0-sas-0x50062b0000000001:1:0-0xd6807184d601e192:0
+a2	pci-0000:02:00.0-sas-0x50062b0000000002:1:1-0xd4905378e6e3d592:1
+a3	pci-0000:02:00.0-sas-0x50062b0000000003:1:2-0xd3827673d806d392:2
+a4	pci-0000:02:00.0-sas-0x50062b0000000004:1:3-0xd6805385d6e3e192:3
+a5	pci-0000:02:00.0-sas-0x50062b0000000005:1:4-0xd680655bd6f5b792:4
+a6	pci-0000:02:00.0-sas-0x50062b0000000006:1:5-0x7a967598ec06d091:5
+a7	pci-0000:02:00.0-sas-0x50062b0000000007:1:6-0xd3826c60d8fcbf92:6
+a8	pci-0000:02:00.0-sas-0x50062b0000000008:1:7-0xd6805271d6e2cd92:7
+
+# Channel B: PCI Bus 3
+b1	pci-0000:03:00.0-sas-0x50062b0000000002:1:0-0xd680685fd6f8bb92:0
+b2	pci-0000:03:00.0-sas-0x50062b0000000003:1:1-0xd58c706de200cb92:1
+b3	pci-0000:03:00.0-sas-0x50062b0000000004:1:2-0xd5897480df04de92:2
+b4	pci-0000:03:00.0-sas-0x50062b0000000005:1:3-0xd6805764d6e7c092:3
+b5	pci-0000:03:00.0-sas-0x50062b0000000006:1:4-0xd6806a6dd6fac992:4
+b6	pci-0000:03:00.0-sas-0x50062b0000000007:1:5-0xd58c6b84e2fbe192:5
+b7	pci-0000:03:00.0-sas-0x50062b0000000008:1:6-0xd58a576ee0e7cb92:6
+b8	pci-0000:03:00.0-sas-0x50062b0000000009:1:7-0xd5877871dd08cf92:7
+
+# Channel C: PCI Bus 4
+c1	pci-0000:04:00.0-sas-0x50062b0000000003:1:0-0xd6806678d6f6d492:0
+c2	pci-0000:04:00.0-sas-0x50062b0000000004:1:1-0xd680696fd6f9cb92:1
+c3	pci-0000:04:00.0-sas-0x50062b0000000005:1:2-0xd3827182d801e292:2
+c4	pci-0000:04:00.0-sas-0x50062b0000000006:1:3-0xd680666fd6f6cb92:3
+c5	pci-0000:04:00.0-sas-0x50062b0000000007:1:4-0xd6806970d6f9cc92:4
+c6	pci-0000:04:00.0-sas-0x50062b0000000008:1:5-0xd6806b62d6fbbe92:5
+c7	pci-0000:04:00.0-sas-0x50062b0000000009:1:6-0xd58a5760e0e7bd92:6
+c8	pci-0000:04:00.0-sas-0x50062b000000000a:1:7-0xd680717fd601dc92:7
+
+# Channel D: PCI Bus 41
+d1	pci-0000:41:00.0-sas-0x50062b0000000004:1:0-0xd6806572d6f5ce92:0
+d2	pci-0000:41:00.0-sas-0x50062b0000000005:1:1-0xd6806983d6f9df92:1
+d3	pci-0000:41:00.0-sas-0x50062b0000000006:1:2-0xd3826c69d8fcc892:2
+d4	pci-0000:41:00.0-sas-0x50062b0000000007:1:3-0xd680725fd602bc92:3
+d5	pci-0000:41:00.0-sas-0x50062b0000000008:1:4-0xd6806971d6f9cd92:4
+d6	pci-0000:41:00.0-sas-0x50062b0000000009:1:5-0xd680726dd602ca92:5
+d7	pci-0000:41:00.0-sas-0x50062b000000000a:1:6-0xd3827772d807d292:6
+d8	pci-0000:41:00.0-sas-0x50062b000000000b:1:7-0xd4955584ebe5e192:7
+
+# Channel E: PCI Bus 42
+e1	pci-0000:42:00.0-sas-0x50062b0000000005:1:0-0xd4925676e8e6d392:0
+e2	pci-0000:42:00.0-sas-0x50062b0000000006:1:1-0xd6806b6ed6fbca92:1
+e3	pci-0000:42:00.0-sas-0x50062b0000000007:1:2-0xd382765fd806bf92:2
+e4	pci-0000:42:00.0-sas-0x50062b0000000008:1:3-0xd587786cdd08ca92:3
+e5	pci-0000:42:00.0-sas-0x50062b0000000009:1:4-0xd66f4e5bc5deb792:4
+e6	pci-0000:42:00.0-sas-0x50062b000000000a:1:5-0xd6806879d6f8d592:5
+e7	pci-0000:42:00.0-sas-0x50062b000000000b:1:6-0xd5885175dee1d292:6
+e8	pci-0000:42:00.0-sas-0x50062b000000000c:1:7-0xd588515fdee1bc92:7
+
+# Channel F: PCI Bus 43
+f1	pci-0000:43:00.0-sas-0x50062b0000000006:1:0-0xd66d6978c3f9d492:0
+f2	pci-0000:43:00.0-sas-0x50062b0000000007:1:1-0xd6806a5cd6fab892:1
+f3	pci-0000:43:00.0-sas-0x50062b0000000008:1:2-0xd6806563d6f5bf92:2
+f4	pci-0000:43:00.0-sas-0x50062b0000000009:1:3-0xd6805664d6e6c092:3
+f5	pci-0000:43:00.0-sas-0x50062b000000000a:1:4-0xd58c707ee200dc92:4
+f6	pci-0000:43:00.0-sas-0x50062b000000000b:1:5-0xd5885781dee7de92:5
+f7	pci-0000:43:00.0-sas-0x50062b000000000c:1:6-0xd3827481d804e192:6
+f8	pci-0000:43:00.0-sas-0x50062b000000000d:1:7-0xd6806863d6f8bf92:7
--- a/include/libzfs.h
+++ b/include/libzfs.h
@@ -64,7 +64,7 @@
  */
 #define	DISK_LABEL_WAIT		(30 * 1000)  /* 30 seconds */
 
-#define	DEFAULT_IMPORT_PATH_SIZE	7
+#define	DEFAULT_IMPORT_PATH_SIZE	8
 extern char *zpool_default_import_path[DEFAULT_IMPORT_PATH_SIZE];
 
 /*
--- a/lib/libzfs/libzfs_import.c
+++ b/lib/libzfs/libzfs_import.c
@@ -1006,6 +1006,7 @@
 char *
 zpool_default_import_path[DEFAULT_IMPORT_PATH_SIZE] = {
 	"/dev/disk/by-vdev",	/* Custom rules, use first if they exist */
+	"/dev/disk/zpool",	/* Custom rules, use first if they exist */
 	"/dev/mapper",		/* Use multipath devices before components */
 	"/dev/disk/by-uuid",	/* Single unique entry and persistent */
 	"/dev/disk/by-id",	/* May be multiple entries and persistent */
--- a/scripts/Makefile.am
+++ b/scripts/Makefile.am
@@ -1,4 +1,4 @@
-SUBDIRS = zpool-config zpios-test zpios-profile
+SUBDIRS = zpool-config zpool-layout zpios-test zpios-profile
 
 EXTRA_DIST = dkms.mkconf dkms.postbuild kmodtool zfs2zol-patch.sed cstyle.pl
 
--- a/scripts/common.sh.in
+++ b/scripts/common.sh.in
@@ -41,7 +41,7 @@
 localstatedir=@localstatedir@
 
 ETCDIR=${ETCDIR:-/etc}
-DEVDIR=${DEVDIR:-/dev/disk/by-vdev}
+DEVDIR=${DEVDIR:-/dev/disk/zpool}
 ZPOOLDIR=${ZPOOLDIR:-${pkgdatadir}/zpool-config}
 ZPIOSDIR=${ZPIOSDIR:-${pkgdatadir}/zpios-test}
 ZPIOSPROFILEDIR=${ZPIOSPROFILEDIR:-${pkgdatadir}/zpios-profile}
@@ -50,6 +50,7 @@
 ZFS=${ZFS:-${sbindir}/zfs}
 ZINJECT=${ZINJECT:-${sbindir}/zinject}
 ZPOOL=${ZPOOL:-${sbindir}/zpool}
+ZPOOL_ID=${ZPOOL_ID:-${bindir}/zpool_id}
 ZTEST=${ZTEST:-${sbindir}/ztest}
 ZPIOS=${ZPIOS:-${sbindir}/zpios}
 
@@ -558,7 +559,7 @@
 
 #
 # The following udev helper functions assume that the provided
-# udev rules file will create a /dev/disk/by-vdev/<CHANNEL><RANK>
+# udev rules file will create a /dev/disk/zpool/<CHANNEL><RANK>
 # disk mapping.  In this mapping each CHANNEL is represented by
 # the letters a-z, and the RANK is represented by the numbers
 # 1-n.  A CHANNEL should identify a group of RANKS which are all
--- a/scripts/zpool-config/zpool-raid0.sh
+++ b/scripts/zpool-config/zpool-raid0.sh
@@ -2,9 +2,10 @@
 #
 # Zpool Raid-0 Configuration
 #
-# This script is used to test with the /dev/disk/by-vdev/[A-Z][1-n] devices.
-# It assumes that you have already populated /dev/disk/by-vdev/ by creating
-# an /etc/zfs/vdev_id.conf file based on your system design.
+# This script is used to simplify testing with the /dev/disk/zpool/[A-Z][1-n]
+# devices.  It assumes that you have already populated /dev/disk/zpool/ by
+# creating an /etc/zfs/zdev.conf file based on your system design.  You may
+# use the zpool_layout command or manually create your own config file.
 #
 # You can then use either the zpool-create.sh or the zpios.sh test script to
 # test various Raid-0 configurations by adjusting the following tunables.
--- a/scripts/zpool-config/zpool-raid10.sh
+++ b/scripts/zpool-config/zpool-raid10.sh
@@ -2,9 +2,10 @@
 #
 # Zpool Raid-10 Configuration
 #
-# This script is used to test with the /dev/disk/by-vdev/[A-Z][1-n] devices.
-# It assumes that you have already populated /dev/disk/by-vdev/ by creating
-# an /etc/zfs/vdev_id.conf file based on your system design.
+# This script is used to simplify testing with the /dev/disk/zpool/[A-Z][1-n]
+# devices.  It assumes that you have already populated /dev/disk/zpool/ by
+# creating an /etc/zfs/zdev.conf file based on your system design.  You may
+# use the zpool_layout command or manually create your own config file.
 #
 # You can then use either the zpool-create.sh or the zpios.sh test script to
 # test various Raid-10 configurations by adjusting the following tunables.
--- a/scripts/zpool-config/zpool-raidz.sh
+++ b/scripts/zpool-config/zpool-raidz.sh
@@ -2,9 +2,10 @@
 #
 # Zpool Raid-Z Configuration
 #
-# This script is used to test with the /dev/disk/by-vdev/[A-Z][1-n] devices.
-# It assumes that you have already populated /dev/disk/by-vdev/ by creating
-# an /etc/zfs/vdev_id.conf file based on your system design.
+# This script is used to simplify testing with the /dev/disk/zpool/[A-Z][1-n]
+# devices.  It assumes that you have already populated /dev/disk/zpool/ by
+# creating an /etc/zfs/zdev.conf file based on your system design.  You may
+# use the zpool_layout command or manually create your own config file.
 #
 # You can then use either the zpool-create.sh or the zpios.sh test script to
 # test various Raid-Z configurations by adjusting the following tunables.
--- /dev/null
+++ b/scripts/zpool-layout/Makefile.am
@@ -0,0 +1,22 @@
+pkglibexecdir = $(libexecdir)/@PACKAGE@/zpool-layout
+dist_pkglibexec_SCRIPTS = \
+	$(top_srcdir)/scripts/zpool-layout/dragon.ddn.conf \
+	$(top_srcdir)/scripts/zpool-layout/dragon.llnl.conf
+
+all:
+	@list='$(dist_pkglibexec_SCRIPTS)'; \
+	for file in $$list; do \
+		link=$$(basename $$file); \
+		if [ ! -e $$link ]; then \
+			$(LN_S) $$file $$link; \
+		fi \
+	done
+
+clean:
+	@list='$(dist_pkglibexec_SCRIPTS)'; \
+	for file in $$list; do \
+		link=$$(basename $$file); \
+		if [ -L $$link ]; then \
+			$(RM) $$link; \
+		fi \
+	done
--- /dev/null
+++ b/scripts/zpool-layout/dragon.ddn.conf
@@ -0,0 +1,67 @@
+#
+# Custom DDN slot mapping for zpool_layout command and
+# StorageScaler 6000 enclosures (Dragon Drawer).
+#
+# Linux	DDN
+# Slot	Slot
+#
+1	1
+2	13
+3	25
+4	37
+5	49
+6	2
+7	14
+8	26
+9	38
+10	50
+11	3
+12	15
+13	27
+14	39
+15	51
+16	4
+17	16
+18	28
+19	40
+20	52
+21	5
+22	17
+23	29
+24	41
+25	53
+26	6
+27	18
+28	30
+29	42
+30	54
+31	7
+32	19
+33	31
+34	43
+35	55
+36	8
+37	20
+38	32
+39	44
+40	56
+41	9
+42	21
+43	33
+44	45
+45	57
+46	10
+47	22
+48	34
+49	46
+50	58
+51	11
+52	23
+53	35
+54	47
+55	59
+56	12
+57	24
+58	36
+59	48
+60	60
--- /dev/null
+++ b/scripts/zpool-layout/dragon.llnl.conf
@@ -0,0 +1,67 @@
+#
+# Custom LLNL slot mapping for zpool_layout command and
+# StorageScaler 6000 enclosures (Dragon Drawer).
+#
+# Linux	LLNL
+# Slot	Slot
+#
+1	25
+2	19
+3	13
+4	7
+5	1
+6	26
+7	20
+8	14
+9	8
+10	2
+11	27
+12	21
+13	15
+14	9
+15	3
+16	28
+17	22
+18	16
+19	10
+20	4
+21	29
+22	23
+23	17
+24	11
+25	5
+26	30
+27	24
+28	18
+29	12
+30	6
+31	60
+32	54
+33	48
+34	42
+35	36
+36	59
+37	53
+38	47
+39	41
+40	35
+41	58
+42	52
+43	46
+44	40
+45	34
+46	57
+47	51
+48	45
+49	39
+50	33
+51	56
+52	50
+53	44
+54	38
+55	32
+56	55
+57	49
+58	43
+59	37
+60	31
--- /dev/null
+++ b/udev/rules.d/60-zpool.rules.in
@@ -0,0 +1,9 @@
+#
+# @udevdir@/rules.d/60-zpool.rules
+#
+
+ENV{DEVTYPE}=="disk", IMPORT{program}="@udevdir@/zpool_id -d %p"
+
+KERNEL=="*[!0-9]", ENV{SUBSYSTEM}=="block", ENV{ID_ZPOOL}=="?*", SYMLINK+="$env{ID_ZPOOL_PATH}"
+KERNEL=="*[0-9]", ENV{SUBSYSTEM}=="block", ENV{DEVTYPE}=="partition", ENV{ID_ZPOOL}=="?*", SYMLINK+="$env{ID_ZPOOL_PATH}-part%n"
+KERNEL=="dm-[0-9]*", ENV{SUBSYSTEM}=="block", ENV{ID_ZPOOL}=="?*", SYMLINK+="$env{ID_ZPOOL_PATH}"
--- a/udev/rules.d/Makefile.am
+++ b/udev/rules.d/Makefile.am
@@ -1,10 +1,12 @@
 udevrule_DATA = \
 	$(top_srcdir)/udev/rules.d/69-vdev.rules \
+	$(top_srcdir)/udev/rules.d/60-zpool.rules \
 	$(top_srcdir)/udev/rules.d/60-zvol.rules \
 	$(top_srcdir)/udev/rules.d/90-zfs.rules
 
 EXTRA_DIST = \
 	$(top_srcdir)/udev/rules.d/69-vdev.rules.in \
+	$(top_srcdir)/udev/rules.d/60-zpool.rules.in \
 	$(top_srcdir)/udev/rules.d/60-zvol.rules.in \
 	$(top_srcdir)/udev/rules.d/90-zfs.rules.in
 
--- a/zfs-script-config.sh.in
+++ b/zfs-script-config.sh.in
@@ -24,6 +24,7 @@
 ZFS=${CMDDIR}/zfs/zfs
 ZINJECT=${CMDDIR}/zinject/zinject
 ZPOOL=${CMDDIR}/zpool/zpool
+ZPOOL_ID=${CMDDIR}/zpool_id/zpool_id
 ZTEST=${CMDDIR}/ztest/ztest
 ZPIOS=${CMDDIR}/zpios/zpios
 
--- a/cmd/Makefile.am
+++ b/cmd/Makefile.am
@@ -1,2 +1,2 @@
 SUBDIRS  = zfs zpool zdb zhack zinject zstreamdump ztest zpios
-SUBDIRS += mount_zfs fsck_zfs zvol_id vdev_id arcstat dbufstat zed
+SUBDIRS += mount_zfs fsck_zfs zpool_layout zvol_id zpool_id vdev_id arcstat dbufstat zed
